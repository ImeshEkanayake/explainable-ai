{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Corpus Processing\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "import nltk.corpus\n",
    "from unidecode                        import unidecode\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import SnowballStemmer\n",
    "from nltk.corpus                      import wordnet as wn\n",
    "from nltk.stem                        import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.preprocessing            import normalize\n",
    "from collections                      import Counter\n",
    "\n",
    "# K-Means\n",
    "from sklearn import cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "import seaborn            as sns\n",
    "from sklearn.datasets                 import make_blobs\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
    "from wordcloud                        import WordCloud\n",
    "from jupyterthemes                    import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Data = pd.read_csv('DsDnsPrScTch.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes a list of words (ie. stopwords) from a tokenized list.\n",
    "def removeWords(listOfTokens, listOfWords):\n",
    "    return [token for token in listOfTokens if token not in listOfWords]\n",
    "\n",
    "# applies stemming to a list of tokenized words\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n",
    "\n",
    "# applies lematization to a list of tokenized words\n",
    "# Annotate text tokens with POS tags\n",
    "wnl = WordNetLemmatizer()\n",
    "def pos_tag_text(tokens):\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    tagged_text = nltk.pos_tag(tokens)\n",
    "\n",
    "    tagged_text_set = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_text_set\n",
    "# lemmatize text based on POS tags    \n",
    "def lemmatize_text(tokens):\n",
    "    \n",
    "    pos_tagged_text = pos_tag_text(tokens)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# remove punctations\n",
    "def removePunctuations(listOfTokens):\n",
    "    test_joined=' '.join(listOfTokens)\n",
    "    test_punc_removed = [char for char in test_joined if char not in string.punctuation]\n",
    "    test_punc_removed_join = ''.join(test_punc_removed)\n",
    "    return [word for word in test_punc_removed_join.split()]\n",
    "\n",
    "# removes any words composed of less than 2 or more than 21 letters\n",
    "def twoLetters(listOfTokens):\n",
    "    twoLetterWord = []\n",
    "    for token in listOfTokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            twoLetterWord.append(token)\n",
    "    return twoLetterWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpus, language):   \n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    #param_stemmer = SnowballStemmer(language)\n",
    "    other_words = [line.rstrip('\\n') for line in open('stopwords_scrapmaker.txt')] # Load .txt file line by line\n",
    "    \n",
    "    for document in corpus:\n",
    "        index = corpus.index(document)\n",
    "        corpus[index] = corpus[index].replace(u'\\ufffd', '8')   # Replaces the ASCII 'ï¿½' symbol with '8'\n",
    "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
    "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
    "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
    "        \n",
    "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
    "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
    "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
    "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
    "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
    "\n",
    "        listOfTokens = word_tokenize(corpus[index])\n",
    "        twoLetterWord = twoLetters(listOfTokens)\n",
    "\n",
    "        listOfTokens = removeWords(listOfTokens, stopwords)\n",
    "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "        \n",
    "        listOfTokens = removePunctuations(listOfTokens)\n",
    "        \n",
    "        #listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
    "        listOfTokens = lemmatize_text(listOfTokens)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "        \n",
    "        corpus[index]   = \" \".join(listOfTokens)\n",
    "        corpus[index] = unidecode(corpus[index])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-81186a542351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtweets_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Project Essay'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Project Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtweets_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Project Essay length'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Project Essay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Project Title length'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Project Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "tweets_df = Data.loc[:,['Project Essay','Project Title']]\n",
    "\n",
    "tweets_df['Project Essay length'] = Data['Project Essay'].apply(len)\n",
    "tweets_df['Project Title length'] = Data['Project Title'].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
